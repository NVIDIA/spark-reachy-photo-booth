# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

import logging
import os

import torch
from configuration import TextToSpeechServiceConfig
from kokoro import KPipeline

from models.base_tts_model import BaseTTSModel


class KokoroTTSModel(BaseTTSModel):
    def __init__(self, config: TextToSpeechServiceConfig, logger: logging.Logger):
        super().__init__(config, logger)

        self.pipeline = KPipeline(
            lang_code="a",
            device=self.device,
            repo_id=self.config.tts_model_config.model_name,
        )
        self.pipeline.g2p.lexicon.golds["photo"] = "foʊdoʊ"  # pyright: ignore[reportAttributeAccessIssue]
        self.pipeline.g2p.lexicon.golds["Photo"] = "foʊdoʊ"  # pyright: ignore[reportAttributeAccessIssue]
        self.pipeline.g2p.lexicon.golds["Reachy"] = "riːtʃi"  # pyright: ignore[reportAttributeAccessIssue]
        self.voices_folder = self._find_voices_folder()
        if self.voices_folder is None:
            raise ValueError("Voices folder not found under Kokoro-82M model directory")

    def _find_voices_folder(self) -> str | None:
        base_dir = "/home/app/.cache/huggingface/hub/models--hexgrad--Kokoro-82M"
        for root, dirs, _ in os.walk(base_dir):
            if "voices" in dirs:
                voices_path = os.path.join(root, "voices")
                self._logger.debug(f"Found voices folder at: {voices_path}")
                return voices_path
        self._logger.warning("Voices folder not found under Kokoro-82M model directory")
        return None

    def generate_audio(self, text: str) -> bytes | None:
        if not text or not text.strip():
            self._logger.warning("Empty or whitespace-only text provided")
            return None

        voice_tensor = torch.load(
            f"{self.voices_folder}/{self.config.tts_model_config.voice_id}.pt",
            weights_only=True,
        )
        generator = self.pipeline(text, voice=voice_tensor)

        audio_chunks = []
        for _, (_, _, audio) in enumerate(generator):
            audio_chunks.append(audio)

        if not audio_chunks:
            self._logger.warning("No audio generated by Kokoro pipeline")
            return None

        voice_data = torch.cat(audio_chunks, dim=0)

        if not isinstance(voice_data, torch.FloatTensor):
            self._logger.error("voice_data is not a FloatTensor")
            return None

        # Convert to desired output format
        voice_data = (voice_data * 32767).to(torch.int16)
        return self.convert_audio(
            voice_data.cpu().numpy(),
            sample_rate_src=24000,
            sample_width_src=2,  # 16-bit = 2 bytes per sample
            channel_count_src=1,
        )
